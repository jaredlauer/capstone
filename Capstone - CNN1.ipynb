{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ec2ef-dbf3-4e9e-a34b-5734a1c22ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f983d688-8462-40a2-aca9-46e106da9da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # import drive from google colab\n",
    "\n",
    "ROOT = \"/content/drive\"     # default location for the drive\n",
    "print(ROOT)                 # print content of ROOT (Optional)\n",
    "\n",
    "drive.mount(ROOT)           # we mount the google drive at /content/drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a90472-f2b1-4553-b98a-cee996578ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pcamlib to Google Colab\n",
    "import imp \n",
    "pcamlib = imp.new_module('pcamlib')\n",
    "exec(open(\"./pcamlib.py\").read(), pcamlib.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4f473-547c-4786-a6ff-b4f5475fcf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pcamlib as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8b9e3-5338-4a2e-abb9-b109e7fc363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic data science packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Import tensorflow packages\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Import various keras tools\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Import tools for model evaluation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,\\\n",
    "roc_curve, roc_auc_score, classification_report, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f683dd9-d704-4910-924a-e35caf9192ff",
   "metadata": {},
   "source": [
    "To get started with this dataset, I adapted the code from this [article](https://geertlitjens.nl/post/getting-started-with-camelyon/) written by Geert Litjens, one of the authors of the dataset.\n",
    "\n",
    "I used his code for the `train_pipeline`, `valid_pipeline`, and `test_pipeline`, which load the train, validation, and test sets and prepare them for modelling. I also make use of his function `convert_sample`. This function extracts each image and its corresponding label from the dataset, converts each image to a TensorFlow `tf.float32` datatype, then performs one-hot encoding on the labels and converts them to `tf.float32` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec86ef6-aa27-4ae1-aa4a-13798efe6f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcam, pcam_info = pc.load_pcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446cd323-a570-421c-97aa-d4ba4ded0783",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline, valid_pipeline, test_pipeline = pc.build_pipelines(pcam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98bd792-c1cb-447e-94dd-709f61e43005",
   "metadata": {},
   "source": [
    "I also used Geert Litjens CNN layer architecture as a starting point. It resembles a VGG16 architecture because it has three sets of two Convolutional layers followed by a single Max Pooling layer, followed by a Flattening layer and two Dense layers before the final Dense layer which outputs the class predictions. I kept the layer parameters the same as his example.\n",
    "\n",
    "I changed the optimizer to `Adam` from `SGD` simply because he provided multiple hyperparameters to go along with it, and I wanted to experiment with that on my own. I also added additional Dropout layers after each convolutional layer, because the first iteration of the model started overfitting quickly after the first epoch and the validation accuracy didn't improve beyond 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a0798-c5c7-4791-9281-7a0ecb129eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model object\n",
    "cnn = Sequential()\n",
    "\n",
    "# Images are 96x96 px, in RGB so there are 3 channels\n",
    "image_shape = (96, 96, 3)\n",
    "\n",
    "# Adding convultional layers to the model \n",
    "# It was important to add dropout layers after each convolutional layer to reduce overfitting\n",
    "cnn.add(Conv2D(16, kernel_size=(3, 3), activation='relu', padding='valid', input_shape=image_shape))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Conv2D(16, kernel_size=(3, 3), activation='relu', padding='valid'))\n",
    "cnn.add(Dropout(0.2))\n",
    "\n",
    "# Add a max pool layer to reduce the dimensions of the feature maps\n",
    "cnn.add(MaxPool2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "# Repeating this architecture two more times\n",
    "cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='valid'))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='valid'))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(MaxPool2D(pool_size=(2, 2), strides=(2,2)))\n",
    "     \n",
    "cnn.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='valid'))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='valid'))\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(MaxPool2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "# Flatten the data to prepare for dense layers\n",
    "cnn.add(Flatten())\n",
    "        \n",
    "cnn.add(Dense(256, activation='relu'))\n",
    "cnn.add(Dropout(0.2))\n",
    "\n",
    "cnn.add(Dense(128, activation='relu'))\n",
    "cnn.add(Dropout(0.2))\n",
    "\n",
    "# Final Dense layer to make class predictions\n",
    "cnn.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ece6c-6985-41f9-a3af-2c61132558f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, this commented line is the original optimizer used in the article:\n",
    "# sgd_opt = SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=True)\n",
    "cnn.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Add early stop callback to prevent the model from overfitting, or running too long\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca35b8-559a-406e-bc84-f1d3736a9676",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = cnn.fit(train_pipeline,\n",
    "                   validation_data=valid_pipeline,\n",
    "                   verbose=1, epochs=15, steps_per_epoch=4096, validation_steps=256,\n",
    "                   callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9988839-67d6-48f3-89f0-2b78381dab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fitted model to a file\n",
    "cnn.save('cnn1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a7310-c4ea-402b-b599-0e15a002cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the history of the model to a csv\n",
    "pc.save_history(history, 'data/models/history/cnn1_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb703a-04c5-4421-8291-e4c35405e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to load the model from file if necessary\n",
    "cnn = tf.keras.models.load_model(\"cnn1\")\n",
    "hist_df = pc.load_history('data/models/history/cnn1_history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff277a38-0a62-49c7-8fad-0f0744f3b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plot_history(hist_df, title='CNN1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477ed0e-4b27-4c01-8259-6d5035f16f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pc.print_test_accuracy(cnn, test_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1751b-5b79-41eb-9a37-fe61d941cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_proba = pc.generate_y_proba(cnn, test_pipeline, class_1=False, save=True, filepath='data/y_proba/cnn1_y_proba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8981c5-e52b-47b0-9bff-d772e27a8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load y_proba from file if not running the model\n",
    "# y_proba = pc.load_y_proba('data/y_proba/cnn1_y_proba.csv')\n",
    "# y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804bf3e1-10c6-462f-998a-ca3a819f0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = pc.generate_y_pred(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1249d61-ab41-439d-80df-1645bfce6017",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_true = pc.generate_y_true(pcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b09de-ab54-4a9b-ab18-a69465dde3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plot_cf_matrix(y_true, y_pred, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea28b17-2f7b-4a15-9df8-17337bee6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.print_classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de8f38-468d-4752-a5f9-453e090c593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plot_roc_curve(y_true, y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5729fb4-2719-4c94-8693-da3baad6c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.plot_misclassified_images(pcam, y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
